# ğŸ”¬ PaperTrail â€” Complete End-to-End Implementation Plan

> **One Line:** Paste any arXiv paper URL â†’ get layered explanations, interactive concept map, related papers, and "what's actually new" â€” in 30 seconds.

---

## ğŸ“ Architecture Overview

```
User Input (arXiv URL or PDF upload)
         â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Agent 1: Paper Parser       â”‚
   â”‚  Extracts structured content â”‚
   â”‚  from PDF â†’ JSON             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Agent 2: Layered Explainer  â”‚
   â”‚  ELI5 / Undergrad / Expert   â”‚
   â”‚  3 complexity levels          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Agent 3: Novelty Detector   â”‚
   â”‚  RAG over related papers     â”‚
   â”‚  What's new vs. incremental  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Agent 4: Concept Mapper     â”‚
   â”‚  Knowledge graph extraction  â”‚
   â”‚  Interactive visualization   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Agent 5: Question Generator â”‚
   â”‚  What it answers + what it   â”‚
   â”‚  leaves open                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
   Clean Streamlit UI with tabs
```

---

## ğŸ› ï¸ Tech Stack â€” All Free

| Component           | Tool                              | Why                                      |
|---------------------|-----------------------------------|------------------------------------------|
| Agent Orchestration | LangGraph                         | Industry standard, conditional routing   |
| LLM                 | Groq + Llama 3.3 70B              | Free, fast, high quality                 |
| Vector DB           | FAISS                             | Local, free, no cloud dependency         |
| Embeddings          | sentence-transformers (all-MiniLM) | Free, runs locally, fast                |
| Paper Fetching      | arXiv API + PyMuPDF               | Free, reliable                           |
| Knowledge Graph     | Pyvis (vis.js wrapper)            | Interactive, browser-native              |
| Frontend            | Streamlit                         | Fast to build, free deployment           |
| Deployment          | Streamlit Community Cloud         | Free forever                             |
| PDF Reports         | ReportLab                         | Free, professional output                |

**Total cost: $0**

---

## ğŸ“ Folder Structure

```
papertrail/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ papers_cache/              # Downloaded PDFs cached here
â”‚   â”œâ”€â”€ faiss_index/               # Pre-built index of CS paper abstracts
â”‚   â””â”€â”€ seed_papers.json           # Seed corpus metadata (arXiv abstracts)
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ paper_parser.py            # Agent 1: PDF â†’ structured JSON
â”‚   â”œâ”€â”€ layered_explainer.py       # Agent 2: 3-level explanations
â”‚   â”œâ”€â”€ novelty_detector.py        # Agent 3: What's new analysis
â”‚   â”œâ”€â”€ concept_mapper.py          # Agent 4: Knowledge graph
â”‚   â””â”€â”€ question_generator.py      # Agent 5: Q&A generation
â”‚
â”œâ”€â”€ graph/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ state.py                   # LangGraph state definition
â”‚   â””â”€â”€ pipeline.py                # LangGraph orchestration
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ arxiv_fetcher.py           # Download papers from arXiv
â”‚   â”œâ”€â”€ pdf_extractor.py           # PyMuPDF text extraction
â”‚   â”œâ”€â”€ embeddings.py              # FAISS + sentence-transformers
â”‚   â”œâ”€â”€ graph_visualizer.py        # Pyvis knowledge graph
â”‚   â””â”€â”€ pdf_report.py              # ReportLab PDF generation
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_seed_index.py        # One-time: build FAISS index from arXiv
â”‚   â””â”€â”€ test_pipeline.py           # End-to-end test script
â”‚
â”œâ”€â”€ app.py                         # Streamlit frontend
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example                   # GROQ_API_KEY placeholder
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ“… 10-Day Build Plan

---

### Day 1 â€” Paper Fetching + PDF Extraction Pipeline

**Goal:** Given an arXiv URL, download the PDF and extract clean structured text.

#### Tasks:
1. **Set up project structure** â€” create all folders, `requirements.txt`, `.env`
2. **Build `arxiv_fetcher.py`:**
   - Accept arXiv URL (handles both `abs/` and `pdf/` URLs)
   - Use `arxiv` Python library to fetch metadata (title, authors, abstract, categories, references)
   - Download PDF to `data/papers_cache/`
   - Return structured metadata dict

3. **Build `pdf_extractor.py`:**
   - Use PyMuPDF (`fitz`) to extract text from PDF
   - Smart section detection: identify Abstract, Introduction, Methodology, Results, Conclusion, References
   - Handle multi-column layouts (common in papers)
   - Clean extracted text: remove headers/footers, page numbers, figure captions
   - Return structured dict:

```python
{
    "title": "Attention Is All You Need",
    "authors": ["Vaswani et al."],
    "abstract": "...",
    "sections": {
        "introduction": "...",
        "methodology": "...",  # or "methods", "approach", "model"
        "results": "...",      # or "experiments", "evaluation"
        "conclusion": "..."
    },
    "references": ["ref1", "ref2", ...],
    "figures_mentioned": ["Figure 1: ...", ...],
    "full_text": "..."  # fallback if section detection fails
}
```

4. **Test:** Fetch "Attention Is All You Need" (arXiv:1706.03762) â†’ verify clean text extraction

#### Key Code â€” `arxiv_fetcher.py`:
```python
import arxiv
import os
import re

class ArxivFetcher:
    def __init__(self, cache_dir="data/papers_cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def parse_arxiv_id(self, url_or_id: str) -> str:
        """Extract arXiv ID from URL or direct ID."""
        patterns = [
            r'arxiv\.org/abs/(\d+\.\d+)',
            r'arxiv\.org/pdf/(\d+\.\d+)',
            r'^(\d+\.\d+)$'
        ]
        for pattern in patterns:
            match = re.search(pattern, url_or_id)
            if match:
                return match.group(1)
        raise ValueError(f"Could not parse arXiv ID from: {url_or_id}")

    def fetch(self, url_or_id: str) -> dict:
        """Fetch paper metadata and PDF."""
        arxiv_id = self.parse_arxiv_id(url_or_id)
        
        # Search for the paper
        search = arxiv.Search(id_list=[arxiv_id])
        paper = next(search.results())
        
        # Download PDF
        pdf_path = os.path.join(self.cache_dir, f"{arxiv_id.replace('/', '_')}.pdf")
        if not os.path.exists(pdf_path):
            paper.download_pdf(dirpath=self.cache_dir, filename=f"{arxiv_id.replace('/', '_')}.pdf")
        
        return {
            "arxiv_id": arxiv_id,
            "title": paper.title,
            "authors": [a.name for a in paper.authors],
            "abstract": paper.summary,
            "categories": paper.categories,
            "published": str(paper.published),
            "pdf_path": pdf_path,
            "pdf_url": paper.pdf_url,
            "entry_url": paper.entry_id
        }
```

#### Key Code â€” `pdf_extractor.py`:
```python
import fitz  # PyMuPDF
import re

class PDFExtractor:
    SECTION_PATTERNS = [
        (r'(?i)^1[\.\s]+introduction', 'introduction'),
        (r'(?i)^2[\.\s]+(?:related\s+work|background)', 'related_work'),
        (r'(?i)^(?:\d+[\.\s]+)?(?:method|approach|model|architecture|proposed)', 'methodology'),
        (r'(?i)^(?:\d+[\.\s]+)?(?:experiment|result|evaluation|empirical)', 'results'),
        (r'(?i)^(?:\d+[\.\s]+)?(?:discussion)', 'discussion'),
        (r'(?i)^(?:\d+[\.\s]+)?(?:conclusion|summary)', 'conclusion'),
        (r'(?i)^(?:references|bibliography)', 'references'),
    ]

    def extract(self, pdf_path: str) -> dict:
        """Extract structured text from a PDF."""
        doc = fitz.open(pdf_path)
        full_text = ""
        for page in doc:
            full_text += page.get_text("text") + "\n"
        doc.close()
        
        # Clean text
        full_text = self._clean_text(full_text)
        
        # Detect sections
        sections = self._detect_sections(full_text)
        
        return {
            "full_text": full_text,
            "sections": sections
        }

    def _clean_text(self, text: str) -> str:
        """Remove noise from extracted text."""
        # Remove page numbers
        text = re.sub(r'\n\d+\n', '\n', text)
        # Remove excessive whitespace
        text = re.sub(r'\n{3,}', '\n\n', text)
        # Remove hyphenation at line breaks
        text = re.sub(r'(\w)-\n(\w)', r'\1\2', text)
        return text.strip()

    def _detect_sections(self, text: str) -> dict:
        """Split text into sections based on headings."""
        sections = {}
        lines = text.split('\n')
        current_section = 'preamble'
        current_content = []
        
        for line in lines:
            matched = False
            for pattern, section_name in self.SECTION_PATTERNS:
                if re.match(pattern, line.strip()):
                    if current_content:
                        sections[current_section] = '\n'.join(current_content).strip()
                    current_section = section_name
                    current_content = []
                    matched = True
                    break
            if not matched:
                current_content.append(line)
        
        if current_content:
            sections[current_section] = '\n'.join(current_content).strip()
        
        return sections
```

**End of Day 1:** You can paste any arXiv URL and get clean, structured text. Foundation done. âœ…

---

### Day 2 â€” Seed Corpus + FAISS Vector Store

**Goal:** Build a searchable index of ~5,000 CS/ML paper abstracts for the Novelty Detector.

#### Tasks:
1. **Build `scripts/build_seed_index.py`:**
   - Use the `arxiv` Python library to batch-fetch abstracts from key CS categories:
     - `cs.AI`, `cs.LG`, `cs.CL`, `cs.CV`, `cs.NE` (covers most ML/AI papers)
   - Fetch the **1,000 most cited / most recent** papers per category
   - Store metadata in `data/seed_papers.json`

2. **Build `utils/embeddings.py`:**
   - Load `all-MiniLM-L6-v2` from sentence-transformers (384-dim, very fast)
   - Generate embeddings for all 5,000 abstracts
   - Build FAISS index (`IndexFlatIP` for cosine similarity)
   - Save index to `data/faiss_index/`
   - Implement `search(query, top_k=10)` method

3. **Test:** Search "transformer self-attention mechanism" â†’ should return Attention paper, BERT, GPT, etc.

#### Key Code â€” `embeddings.py`:
```python
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import json
import os

class PaperIndex:
    def __init__(self, index_dir="data/faiss_index", model_name="all-MiniLM-L6-v2"):
        self.index_dir = index_dir
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.metadata = []
        os.makedirs(index_dir, exist_ok=True)

    def build_index(self, papers: list[dict]):
        """Build FAISS index from paper abstracts."""
        texts = [f"{p['title']}. {p['abstract']}" for p in papers]
        embeddings = self.model.encode(texts, show_progress_bar=True, normalize_embeddings=True)
        
        dim = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dim)  # Inner product = cosine for normalized vectors
        self.index.add(embeddings.astype(np.float32))
        self.metadata = papers
        
        # Save
        faiss.write_index(self.index, os.path.join(self.index_dir, "papers.index"))
        with open(os.path.join(self.index_dir, "metadata.json"), "w") as f:
            json.dump(self.metadata, f)
        
        print(f"Indexed {len(papers)} papers. Dimension: {dim}")

    def load_index(self):
        """Load existing FAISS index."""
        self.index = faiss.read_index(os.path.join(self.index_dir, "papers.index"))
        with open(os.path.join(self.index_dir, "metadata.json")) as f:
            self.metadata = json.load(f)

    def search(self, query: str, top_k: int = 10) -> list[dict]:
        """Search for similar papers."""
        query_embedding = self.model.encode([query], normalize_embeddings=True)
        scores, indices = self.index.search(query_embedding.astype(np.float32), top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.metadata):
                result = self.metadata[idx].copy()
                result["similarity_score"] = float(score)
                results.append(result)
        return results
```

#### Key Code â€” `scripts/build_seed_index.py`:
```python
import arxiv
import json
from utils.embeddings import PaperIndex

CATEGORIES = ["cs.AI", "cs.LG", "cs.CL", "cs.CV", "cs.NE"]
PAPERS_PER_CATEGORY = 1000

def fetch_papers():
    all_papers = []
    seen_ids = set()
    
    for category in CATEGORIES:
        print(f"Fetching {category}...")
        search = arxiv.Search(
            query=f"cat:{category}",
            max_results=PAPERS_PER_CATEGORY,
            sort_by=arxiv.SortCriterion.Relevance
        )
        for paper in search.results():
            if paper.entry_id not in seen_ids:
                seen_ids.add(paper.entry_id)
                all_papers.append({
                    "arxiv_id": paper.entry_id,
                    "title": paper.title,
                    "abstract": paper.summary,
                    "authors": [a.name for a in paper.authors],
                    "categories": paper.categories,
                    "published": str(paper.published)
                })
    
    print(f"Total unique papers: {len(all_papers)}")
    return all_papers

if __name__ == "__main__":
    papers = fetch_papers()
    
    # Save raw data
    with open("data/seed_papers.json", "w") as f:
        json.dump(papers, f, indent=2)
    
    # Build FAISS index
    indexer = PaperIndex()
    indexer.build_index(papers)
    print("Done! Index saved to data/faiss_index/")
```

**End of Day 2:** You have a searchable knowledge base of 5,000 ML papers. âœ…

---

### Day 3 â€” Agent 1: Paper Parser + Agent 2: Layered Explainer

**Goal:** Extract structured content and generate 3-level explanations.

#### Agent 1 â€” Paper Parser (`agents/paper_parser.py`):
This agent combines the arXiv fetcher + PDF extractor into a clean pipeline, and uses the LLM to enhance section detection when regex fails.

```python
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from utils.arxiv_fetcher import ArxivFetcher
from utils.pdf_extractor import PDFExtractor

class ParsedPaper(BaseModel):
    title: str
    authors: list[str]
    abstract: str
    problem_statement: str = Field(description="The core problem this paper addresses")
    methodology_summary: str = Field(description="Key methodology in 3-5 sentences")
    key_results: list[str] = Field(description="Top 3-5 results/findings")
    limitations: list[str] = Field(description="Acknowledged limitations")
    key_terms: list[str] = Field(description="Important technical terms used")
    paper_type: str = Field(description="theoretical/empirical/survey/benchmark")

class PaperParserAgent:
    def __init__(self):
        self.fetcher = ArxivFetcher()
        self.extractor = PDFExtractor()
        self.llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0)

    def parse(self, arxiv_url: str) -> dict:
        # Step 1: Fetch metadata + PDF
        metadata = self.fetcher.fetch(arxiv_url)
        
        # Step 2: Extract text from PDF
        extracted = self.extractor.extract(metadata["pdf_path"])
        
        # Step 3: LLM-enhanced parsing for structured fields
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a research paper analyst. Extract structured information 
            from this paper. Be precise and factual â€” only state what the paper actually claims."""),
            ("user", """Paper Title: {title}
            
            Abstract: {abstract}
            
            Full Text (first 6000 chars): {text_excerpt}
            
            Extract the structured information as requested.""")
        ])
        
        structured_llm = self.llm.with_structured_output(ParsedPaper)
        
        result = structured_llm.invoke(
            prompt.format_messages(
                title=metadata["title"],
                abstract=metadata["abstract"],
                text_excerpt=extracted["full_text"][:6000]
            )
        )
        
        return {
            "metadata": metadata,
            "extracted_text": extracted,
            "parsed": result.model_dump()
        }
```

#### Agent 2 â€” Layered Explainer (`agents/layered_explainer.py`):
```python
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

class LayeredExplanation(BaseModel):
    eli5: str = Field(description="Explanation a 5-year-old could understand. Use analogies. Max 4 sentences.")
    undergrad: str = Field(description="Explanation for a CS undergrad. Can use basic ML terms. 6-8 sentences.")
    expert: str = Field(description="Technical explanation for an ML researcher. Include specific techniques. 8-10 sentences.")
    one_sentence: str = Field(description="The paper in exactly one sentence.")
    key_insight: str = Field(description="The single most important insight from this paper.")

class LayeredExplainerAgent:
    def __init__(self):
        self.llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0.3)

    def explain(self, parsed_paper: dict) -> dict:
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a world-class science communicator who can explain any 
            research paper at multiple complexity levels. Your explanations are:
            - ELI5: Uses everyday analogies, no jargon, a child could understand
            - Undergrad: Uses basic CS/ML terminology, assumes linear algebra and probability knowledge
            - Expert: Uses precise technical language, references related work, discusses implications
            
            Be specific to THIS paper â€” don't give generic explanations."""),
            ("user", """Paper: {title}
            
            Abstract: {abstract}
            
            Problem: {problem}
            
            Methodology: {methodology}
            
            Key Results: {results}
            
            Generate layered explanations.""")
        ])
        
        structured_llm = self.llm.with_structured_output(LayeredExplanation)
        
        result = structured_llm.invoke(
            prompt.format_messages(
                title=parsed_paper["parsed"]["title"],
                abstract=parsed_paper["metadata"]["abstract"],
                problem=parsed_paper["parsed"]["problem_statement"],
                methodology=parsed_paper["parsed"]["methodology_summary"],
                results="\n".join(parsed_paper["parsed"]["key_results"])
            )
        )
        
        return result.model_dump()
```

**End of Day 3:** Paste a paper URL â†’ get structured parse + 3 explanation levels. âœ…

---

### Day 4 â€” Agent 3: Novelty Detector + Agent 4: Concept Mapper

**Goal:** Identify what's genuinely new + build an interactive knowledge graph.

#### Agent 3 â€” Novelty Detector (`agents/novelty_detector.py`):
```python
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from utils.embeddings import PaperIndex

class NoveltyAnalysis(BaseModel):
    novel_contributions: list[str] = Field(description="What this paper introduces that didn't exist before")
    incremental_improvements: list[str] = Field(description="What this paper improves upon existing work")
    builds_upon: list[str] = Field(description="Key prior work this paper extends")
    novelty_score: int = Field(description="1-10 how novel is this paper", ge=1, le=10)
    novelty_summary: str = Field(description="2-3 sentence summary of what's actually new")

class NoveltyDetectorAgent:
    def __init__(self):
        self.llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0)
        self.paper_index = PaperIndex()
        self.paper_index.load_index()

    def detect(self, parsed_paper: dict) -> dict:
        # Step 1: Find related papers via FAISS
        query = f"{parsed_paper['parsed']['title']}. {parsed_paper['parsed']['problem_statement']}"
        related_papers = self.paper_index.search(query, top_k=10)
        
        # Step 2: Build context from related papers
        related_context = "\n\n".join([
            f"- {p['title']} ({p['published'][:4]}): {p['abstract'][:300]}..."
            for p in related_papers
        ])
        
        # Step 3: LLM novelty analysis
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a senior ML researcher reviewing a paper for novelty. 
            Compare it against related prior work and identify:
            1. What's genuinely NEW (novel contribution)
            2. What's an incremental improvement over existing work
            3. What prior work it builds upon
            Be brutally honest â€” if the contribution is incremental, say so."""),
            ("user", """Paper under review:
            Title: {title}
            Problem: {problem}
            Methodology: {methodology}
            Results: {results}
            
            Related prior work found in the literature:
            {related_papers}
            
            Analyze the novelty of this paper.""")
        ])
        
        structured_llm = self.llm.with_structured_output(NoveltyAnalysis)
        result = structured_llm.invoke(
            prompt.format_messages(
                title=parsed_paper["parsed"]["title"],
                problem=parsed_paper["parsed"]["problem_statement"],
                methodology=parsed_paper["parsed"]["methodology_summary"],
                results="\n".join(parsed_paper["parsed"]["key_results"]),
                related_papers=related_context
            )
        )
        
        return {
            "novelty": result.model_dump(),
            "related_papers": related_papers[:5]  # Top 5 for display
        }
```

#### Agent 4 â€” Concept Mapper (`agents/concept_mapper.py`):
```python
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

class ConceptNode(BaseModel):
    id: str = Field(description="Unique short identifier")
    label: str = Field(description="Display name of the concept")
    category: str = Field(description="One of: method, dataset, metric, concept, result, problem")
    importance: int = Field(description="1-5, how central to the paper", ge=1, le=5)

class ConceptEdge(BaseModel):
    source: str = Field(description="Source node id")
    target: str = Field(description="Target node id")
    relationship: str = Field(description="e.g., 'uses', 'improves', 'evaluates_on', 'produces', 'solves'")

class ConceptMap(BaseModel):
    nodes: list[ConceptNode]
    edges: list[ConceptEdge]

class ConceptMapperAgent:
    def __init__(self):
        self.llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0)

    def map_concepts(self, parsed_paper: dict) -> dict:
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a knowledge graph specialist. Extract the key concepts 
            from this research paper and their relationships. Create a concept map with:
            - 10-20 nodes (key concepts, methods, datasets, metrics)
            - Meaningful edges showing relationships between concepts
            - Proper categorization of each node
            
            Focus on the MOST important concepts. Every node should connect to at least one other node."""),
            ("user", """Paper: {title}
            
            Abstract: {abstract}
            
            Methodology: {methodology}
            
            Key Terms: {key_terms}
            
            Key Results: {results}
            
            Generate the concept map.""")
        ])
        
        structured_llm = self.llm.with_structured_output(ConceptMap)
        result = structured_llm.invoke(
            prompt.format_messages(
                title=parsed_paper["parsed"]["title"],
                abstract=parsed_paper["metadata"]["abstract"],
                methodology=parsed_paper["parsed"]["methodology_summary"],
                key_terms=", ".join(parsed_paper["parsed"]["key_terms"]),
                results="\n".join(parsed_paper["parsed"]["key_results"])
            )
        )
        
        return result.model_dump()
```

#### Knowledge Graph Visualization (`utils/graph_visualizer.py`):
```python
from pyvis.network import Network
import json

# Color scheme per category
CATEGORY_COLORS = {
    "method": "#6366f1",     # Indigo
    "dataset": "#f59e0b",    # Amber
    "metric": "#10b981",     # Emerald
    "concept": "#3b82f6",    # Blue
    "result": "#ef4444",     # Red
    "problem": "#8b5cf6",    # Violet
}

def build_knowledge_graph(concept_map: dict, output_path: str = "concept_graph.html"):
    net = Network(height="500px", width="100%", bgcolor="#0f172a", font_color="white")
    net.barnes_hut(gravity=-3000, central_gravity=0.3, spring_length=200)
    
    for node in concept_map["nodes"]:
        color = CATEGORY_COLORS.get(node["category"], "#94a3b8")
        size = 15 + (node["importance"] * 8)
        net.add_node(
            node["id"], 
            label=node["label"], 
            color=color, 
            size=size,
            title=f"{node['label']} ({node['category']})"
        )
    
    for edge in concept_map["edges"]:
        net.add_edge(
            edge["source"], 
            edge["target"], 
            label=edge["relationship"],
            color="#475569",
            arrows="to"
        )
    
    net.save_graph(output_path)
    return output_path
```

**End of Day 4:** Full novelty analysis + interactive knowledge graph. âœ…

---

### Day 5 â€” Agent 5: Question Generator + LangGraph Pipeline

**Goal:** Final agent + wire everything into LangGraph.

#### Agent 5 â€” Question Generator (`agents/question_generator.py`):
```python
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

class PaperQuestions(BaseModel):
    questions_answered: list[str] = Field(
        description="5-7 specific research questions this paper answers"
    )
    questions_left_open: list[str] = Field(
        description="3-5 questions this paper does NOT answer or leaves for future work"
    )
    follow_up_reading: list[str] = Field(
        description="3-5 suggested topics/papers to read next to deepen understanding"
    )
    discussion_questions: list[str] = Field(
        description="3 thought-provoking questions for a reading group discussion"
    )

class QuestionGeneratorAgent:
    def __init__(self):
        self.llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0.3)

    def generate(self, parsed_paper: dict, novelty: dict) -> dict:
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a research mentor helping a student deeply understand a paper.
            Generate insightful questions that:
            1. Highlight what this paper actually answers
            2. Point out gaps and future directions
            3. Suggest logical next readings
            4. Spark thoughtful discussion
            
            Questions should be specific to THIS paper, not generic."""),
            ("user", """Paper: {title}
            Problem: {problem}
            Methodology: {methodology}
            Results: {results}
            Limitations: {limitations}
            Novel contributions: {novel}
            
            Generate questions.""")
        ])
        
        structured_llm = self.llm.with_structured_output(PaperQuestions)
        result = structured_llm.invoke(
            prompt.format_messages(
                title=parsed_paper["parsed"]["title"],
                problem=parsed_paper["parsed"]["problem_statement"],
                methodology=parsed_paper["parsed"]["methodology_summary"],
                results="\n".join(parsed_paper["parsed"]["key_results"]),
                limitations="\n".join(parsed_paper["parsed"]["limitations"]),
                novel="\n".join(novelty["novelty"]["novel_contributions"])
            )
        )
        return result.model_dump()
```

#### LangGraph State + Pipeline (`graph/state.py` and `graph/pipeline.py`):

```python
# graph/state.py
from typing import TypedDict, Optional

class PaperTrailState(TypedDict):
    # Input
    arxiv_url: str
    
    # Agent 1 output
    parsed_paper: Optional[dict]
    
    # Agent 2 output
    explanations: Optional[dict]
    
    # Agent 3 output
    novelty_analysis: Optional[dict]
    related_papers: Optional[list]
    
    # Agent 4 output
    concept_map: Optional[dict]
    graph_html_path: Optional[str]
    
    # Agent 5 output
    questions: Optional[dict]
    
    # Control
    error: Optional[str]
    status: str  # "processing", "complete", "error"
```

```python
# graph/pipeline.py
from langgraph.graph import StateGraph, END
from graph.state import PaperTrailState
from agents.paper_parser import PaperParserAgent
from agents.layered_explainer import LayeredExplainerAgent
from agents.novelty_detector import NoveltyDetectorAgent
from agents.concept_mapper import ConceptMapperAgent
from agents.question_generator import QuestionGeneratorAgent
from utils.graph_visualizer import build_knowledge_graph

# Initialize agents
parser = PaperParserAgent()
explainer = LayeredExplainerAgent()
novelty_detector = NoveltyDetectorAgent()
concept_mapper = ConceptMapperAgent()
question_gen = QuestionGeneratorAgent()

def parse_paper_node(state: PaperTrailState) -> dict:
    try:
        result = parser.parse(state["arxiv_url"])
        return {"parsed_paper": result, "status": "processing"}
    except Exception as e:
        return {"error": str(e), "status": "error"}

def explain_paper_node(state: PaperTrailState) -> dict:
    result = explainer.explain(state["parsed_paper"])
    return {"explanations": result}

def detect_novelty_node(state: PaperTrailState) -> dict:
    result = novelty_detector.detect(state["parsed_paper"])
    return {
        "novelty_analysis": result["novelty"],
        "related_papers": result["related_papers"]
    }

def map_concepts_node(state: PaperTrailState) -> dict:
    result = concept_mapper.map_concepts(state["parsed_paper"])
    graph_path = build_knowledge_graph(result)
    return {"concept_map": result, "graph_html_path": graph_path}

def generate_questions_node(state: PaperTrailState) -> dict:
    result = question_gen.generate(
        state["parsed_paper"],
        {"novelty": state["novelty_analysis"]}
    )
    return {"questions": result, "status": "complete"}

def should_continue(state: PaperTrailState) -> str:
    if state.get("error"):
        return "error"
    return "continue"

# Build the graph
def build_pipeline():
    workflow = StateGraph(PaperTrailState)
    
    # Add nodes
    workflow.add_node("parse_paper", parse_paper_node)
    workflow.add_node("explain_paper", explain_paper_node)
    workflow.add_node("detect_novelty", detect_novelty_node)
    workflow.add_node("map_concepts", map_concepts_node)
    workflow.add_node("generate_questions", generate_questions_node)
    
    # Define edges
    workflow.set_entry_point("parse_paper")
    
    workflow.add_conditional_edges(
        "parse_paper",
        should_continue,
        {"continue": "explain_paper", "error": END}
    )
    
    # Explain and detect novelty can run in parallel (both depend only on parsed_paper)
    workflow.add_edge("explain_paper", "detect_novelty")
    workflow.add_edge("detect_novelty", "map_concepts")
    workflow.add_edge("map_concepts", "generate_questions")
    workflow.add_edge("generate_questions", END)
    
    return workflow.compile()

# Main entry point
pipeline = build_pipeline()

def analyze_paper(arxiv_url: str) -> dict:
    """Run the full PaperTrail pipeline."""
    result = pipeline.invoke({
        "arxiv_url": arxiv_url,
        "status": "processing"
    })
    return result
```

**End of Day 5:** Full pipeline works end-to-end. `analyze_paper("1706.03762")` returns everything. âœ…

---

### Day 6 â€” Streamlit UI

**Goal:** Beautiful, tabbed interface that showcases all 5 agents' output.

#### UI Layout Design:
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ”¬ PaperTrail                                  â•‘
â•‘  Understand any research paper in 30 seconds    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                  â•‘
â•‘  Paste arXiv URL:                               â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘  â”‚ https://arxiv.org/abs/1706.03762         â”‚   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•‘                                                  â•‘
â•‘  [ğŸ” Analyze Paper]                             â•‘
â•‘                                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                  â•‘
â•‘  ğŸ“„ Paper Info  (collapsible header card)        â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘  â”‚ Attention Is All You Need                â”‚   â•‘
â•‘  â”‚ Vaswani et al. | 2017 | cs.CL           â”‚   â•‘
â•‘  â”‚ Type: Empirical | Terms: transformer...  â”‚   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•‘                                                  â•‘
â•‘  TABS:                                           â•‘
â•‘  [ğŸ“– Explanations] [ğŸ†• Novelty] [ğŸ•¸ï¸ Concept Map] [â“ Questions] â•‘
â•‘                                                  â•‘
â•‘  â”Œâ”€â”€EXPLANATIONS TABâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â•‘
â•‘  â”‚                                           â”‚   â•‘
â•‘  â”‚  ğŸ’¡ One Sentence: "This paper..."        â”‚   â•‘
â•‘  â”‚                                           â”‚   â•‘
â•‘  â”‚  ğŸ§’ ELI5                                 â”‚   â•‘
â•‘  â”‚  Imagine you're reading a book...         â”‚   â•‘
â•‘  â”‚                                           â”‚   â•‘
â•‘  â”‚  ğŸ“ Undergrad                             â”‚   â•‘
â•‘  â”‚  The transformer architecture...          â”‚   â•‘
â•‘  â”‚                                           â”‚   â•‘
â•‘  â”‚  ğŸ”¬ Expert                                â”‚   â•‘
â•‘  â”‚  This work introduces a novel...          â”‚   â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â•‘
â•‘                                                  â•‘
â•‘  [ğŸ“¥ Download PDF Report]                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

#### Key Streamlit Code (`app.py`):
```python
import streamlit as st
from graph.pipeline import analyze_paper
import streamlit.components.v1 as components

st.set_page_config(page_title="PaperTrail", page_icon="ğŸ”¬", layout="wide")

# Custom CSS for premium look
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
    
    .stApp { font-family: 'Inter', sans-serif; }
    
    .paper-card {
        background: linear-gradient(135deg, #1e1b4b 0%, #312e81 100%);
        border-radius: 16px;
        padding: 24px;
        color: white;
        margin: 16px 0;
    }
    
    .explanation-card {
        background: #f8fafc;
        border-left: 4px solid;
        border-radius: 8px;
        padding: 16px;
        margin: 12px 0;
    }
    
    .eli5 { border-color: #f59e0b; }
    .undergrad { border-color: #3b82f6; }
    .expert { border-color: #8b5cf6; }
    
    .novelty-badge {
        display: inline-block;
        padding: 4px 12px;
        border-radius: 20px;
        font-size: 14px;
        font-weight: 600;
    }
    .novel { background: #dcfce7; color: #166534; }
    .incremental { background: #fef3c7; color: #92400e; }
</style>
""", unsafe_allow_html=True)

# Header
st.markdown("# ğŸ”¬ PaperTrail")
st.markdown("### Understand any research paper in 30 seconds")

# Input
url = st.text_input("Paste an arXiv URL:", placeholder="https://arxiv.org/abs/1706.03762")

if st.button("ğŸ” Analyze Paper", type="primary", use_container_width=True):
    with st.spinner("ğŸ¤– 5 AI agents analyzing your paper..."):
        result = analyze_paper(url)
    
    if result.get("error"):
        st.error(f"Error: {result['error']}")
    else:
        # Paper info card
        p = result["parsed_paper"]["parsed"]
        st.markdown(f"""
        <div class="paper-card">
            <h2>{p['title']}</h2>
            <p>{', '.join(result['parsed_paper']['metadata']['authors'][:5])}</p>
            <p>ğŸ“… {result['parsed_paper']['metadata']['published'][:10]} | 
               ğŸ·ï¸ {', '.join(result['parsed_paper']['metadata']['categories'])} |
               ğŸ“ {p['paper_type']}</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Tabs
        tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“– Explanations", "ğŸ†• What's New", "ğŸ•¸ï¸ Concept Map", "â“ Questions"])
        
        with tab1:
            exp = result["explanations"]
            st.markdown(f"**ğŸ’¡ In one sentence:** {exp['one_sentence']}")
            st.markdown(f"**ğŸ”‘ Key insight:** {exp['key_insight']}")
            st.divider()
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown("### ğŸ§’ ELI5")
                st.info(exp["eli5"])
            with col2:
                st.markdown("### ğŸ“ Undergrad")
                st.warning(exp["undergrad"])
            with col3:
                st.markdown("### ğŸ”¬ Expert")
                st.success(exp["expert"])
        
        with tab2:
            nov = result["novelty_analysis"]
            score = nov["novelty_score"]
            st.metric("Novelty Score", f"{score}/10")
            st.markdown(f"**Summary:** {nov['novelty_summary']}")
            
            st.markdown("#### âœ¨ Novel Contributions")
            for c in nov["novel_contributions"]:
                st.markdown(f'<span class="novelty-badge novel">NEW</span> {c}', unsafe_allow_html=True)
            
            st.markdown("#### ğŸ“ˆ Incremental Improvements")
            for c in nov["incremental_improvements"]:
                st.markdown(f'<span class="novelty-badge incremental">IMPROVED</span> {c}', unsafe_allow_html=True)
            
            st.markdown("#### ğŸ“š Related Papers")
            for p in result.get("related_papers", []):
                st.markdown(f"- **{p['title']}** (similarity: {p['similarity_score']:.2f})")
        
        with tab3:
            # Render Pyvis HTML
            if result.get("graph_html_path"):
                with open(result["graph_html_path"], "r") as f:
                    graph_html = f.read()
                components.html(graph_html, height=550, scrolling=True)
        
        with tab4:
            q = result["questions"]
            st.markdown("#### âœ… Questions This Paper Answers")
            for question in q["questions_answered"]:
                st.markdown(f"- {question}")
            
            st.markdown("#### ğŸ”® Questions Left Open")
            for question in q["questions_left_open"]:
                st.markdown(f"- {question}")
            
            st.markdown("#### ğŸ“– Suggested Follow-Up Reading")
            for suggestion in q["follow_up_reading"]:
                st.markdown(f"- {suggestion}")
            
            st.markdown("#### ğŸ’¬ Discussion Questions")
            for question in q["discussion_questions"]:
                st.markdown(f"- â“ {question}")
```

**End of Day 6:** Beautiful UI with all 5 agent outputs in tabs. âœ…

---

### Day 7 â€” PDF Report + Polish

**Goal:** Downloadable report + UI refinements.

#### Tasks:
1. **Build `utils/pdf_report.py`** â€” generate a professional PDF report containing:
   - Paper metadata (title, authors, date)
   - All 3 explanation levels
   - Novelty analysis with score
   - Questions answered and left open
   - Related papers list
   - Generated date and PaperTrail branding

2. **Add download button** to Streamlit UI

3. **Add analysis history** â€” use `st.session_state` to let users analyze multiple papers in one session, with a sidebar showing past analyses

4. **Add example papers** â€” pre-load 3-5 famous papers as quick-select buttons:
   - "Attention Is All You Need" (1706.03762)
   - "BERT" (1810.04805)
   - "GPT-3 / Language Models are Few-Shot Learners" (2005.14165)
   - "ResNet" (1512.03385)
   - "Diffusion Models Beat GANs" (2105.05233)

5. **Error handling** â€” graceful handling of invalid URLs, rate limits, empty papers

**End of Day 7:** Polished app with PDF download + example papers. âœ…

---

### Day 8 â€” Testing + Edge Cases

**Goal:** Make sure it works reliably across different paper types.

#### Test Matrix:

| Paper Type | Example | What to Test |
|-----------|---------|-------------|
| Seminal ML paper | Attention (1706.03762) | All agents work well |
| Recent paper | Any 2024/2025 paper | Handles new content |
| Survey paper | "A Survey of LLMs" | Different section structure |
| Short paper (4 pages) | Workshop paper | Handles limited content |
| Math-heavy paper | Theoretical paper | Handles equations |
| Non-ML CS paper | Systems/networking | Works outside ML |

#### Specific Tests:
```python
# scripts/test_pipeline.py
TEST_PAPERS = [
    "1706.03762",  # Attention Is All You Need
    "1810.04805",  # BERT
    "2005.14165",  # GPT-3
    "2303.08774",  # GPT-4 Technical Report
    "2010.11929",  # ViT
]

for paper_id in TEST_PAPERS:
    print(f"\nTesting {paper_id}...")
    result = analyze_paper(paper_id)
    
    # Assertions
    assert result["status"] == "complete", f"Failed: {result.get('error')}"
    assert len(result["explanations"]["eli5"]) > 50, "ELI5 too short"
    assert len(result["concept_map"]["nodes"]) >= 5, "Too few concepts"
    assert result["novelty_analysis"]["novelty_score"] >= 1, "Invalid score"
    
    print(f"  âœ… {result['parsed_paper']['parsed']['title']}")
```

**End of Day 8:** Reliable across diverse papers. âœ…

---

### Day 9 â€” Deploy

#### Steps:
```
1. Create requirements.txt with pinned versions
2. Create .streamlit/config.toml for theme
3. Push to GitHub (public repo)
4. Go to share.streamlit.io â†’ connect repo
5. Set GROQ_API_KEY in Streamlit Secrets
6. Deploy â†’ get live URL
7. Test on mobile + different browsers
8. Share on LinkedIn/Twitter with a demo GIF
```

#### `requirements.txt`:
```
streamlit>=1.30.0
langchain>=0.2.0
langchain-groq>=0.1.0
langgraph>=0.1.0
sentence-transformers>=2.2.0
faiss-cpu>=1.7.4
arxiv>=2.1.0
PyMuPDF>=1.23.0
pyvis>=0.3.2
reportlab>=4.0.0
pydantic>=2.0.0
python-dotenv>=1.0.0
```

**End of Day 9:** Live at `papertrail.streamlit.app` âœ…

---

### Day 10 â€” README + Interview Prep

#### README Structure:
```markdown
# ğŸ”¬ PaperTrail â€” Research Paper Understanding Engine

> Paste any arXiv paper â†’ get layered explanations, novelty analysis, 
> concept maps, and research questions in 30 seconds.

## ğŸ¯ Problem
Research papers are dense, jargon-heavy, and time-consuming...

## ğŸ—ï¸ Architecture  
[Architecture diagram]
5 specialized AI agents orchestrated via LangGraph...

## ğŸš€ Live Demo
[papertrail.streamlit.app](https://papertrail.streamlit.app)

## ğŸ“¸ Screenshots
[Show each tab - Explanations, Novelty, Concept Map, Questions]

## ğŸ› ï¸ Tech Stack
[Table of technologies]

## ğŸ“Š Results
- Tested on 50+ papers across CS subfields
- Average analysis time: ~25 seconds
- Knowledge graph accuracy validated against manual concept extraction
```

#### Interview Answer (45 seconds):
*"I built PaperTrail because reading research papers takes hours and most people give up after the abstract. You paste any arXiv URL and 5 agents work together â€” one parses the PDF into structured content, one generates explanations at 3 complexity levels from ELI5 to expert, one uses RAG over 5,000 papers to identify what's genuinely novel versus incremental, one builds an interactive knowledge graph, and one generates research questions the paper answers and leaves open. It uses LangGraph for orchestration, FAISS for semantic search, and Groq for inference. It's live and been used by 100+ researchers."*

---

## âš¡ Critical Risk Mitigations

| Risk | Impact | Mitigation |
|------|--------|-----------|
| Groq rate limits | Pipeline stalls | Batch agent calls, add retry with exponential backoff, cache results |
| PDF extraction fails (scanned PDFs) | No text to analyze | Fall back to abstract-only mode, notify user |
| FAISS index too large for Streamlit Cloud | Deployment fails | Limit seed corpus to ~3,000 papers, use quantized index |
| Long papers exceed context window | Truncated analysis | Chunk and summarize methodology section before sending to LLM |
| Concept graph too messy | Bad visualization | Limit to 15 nodes max, force hierarchical layout |

---

## ğŸ¯ Stretch Goals (If You Finish Early)

1. **Paper Comparison Mode** â€” Analyze 2 papers side-by-side, show how they relate
2. **Reading List Generator** â€” Given a topic, generate a curated 10-paper reading list with order
3. **Citation Graph** â€” Visualize which papers cite which (use Semantic Scholar API, free)
4. **Collaborative Annotations** â€” Users can add notes to papers (store in session)
5. **Weekly arXiv Digest** â€” Auto-analyze top 5 papers of the week in your chosen category
